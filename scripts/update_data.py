"""
@file update_data.py
@brief Script principal do processo ETL (Extra√ß√£o, Transforma√ß√£o, Carga) de criptomoedas.

Este script √© respons√°vel por automatizar a coleta, processamento e armazenamento de dados
de criptomoedas da API CoinGecko, utilizando Redis para cache e armazenamento. Ele foi projetado para ser
robusto, com tratamento de erros, retries, caching e processamento em lotes, garantindo
uma atualiza√ß√£o eficiente e cont√≠nua dos dados de mercado.

Funcionalidades principais:
- Extra√ß√£o (Extract): Coleta dados de mercado de criptomoedas da CoinGecko, utilizando cache Redis.
- Transforma√ß√£o (Transform): Valida, limpa e padroniza os dados brutos.
- Armazenamento (Store): Armazena os dados processados no Redis para acesso r√°pido.
- Cache Inteligente: Gerencia automaticamente o cache para otimizar o uso da API.
- Ciclo Cont√≠nuo: Pode ser executado em um loop cont√≠nuo para manter os dados atualizados em intervalos regulares.

Tecnologias Utilizadas:
- httpx: Cliente HTTP ass√≠ncrono para fazer requisi√ß√µes √† API da CoinGecko.
- Redis (via redis.asyncio): Para cache e armazenamento de dados em mem√≥ria.
- dotenv: Para carregar vari√°veis de ambiente de forma segura.
"""

import logging
import asyncio
import json
import os
import sys
import time
from pathlib import Path
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple, Union

# httpx: Cliente HTTP ass√≠ncrono de terceiros para fazer requisi√ß√µes web.
import httpx
# load_dotenv: Fun√ß√£o para carregar vari√°veis de ambiente de um arquivo .env.
from dotenv import load_dotenv
# aioredis: Cliente Redis ass√≠ncrono para Python.
import redis.asyncio as aioredis

# Configura√ß√£o de logging:
# Configura o sistema de log para este script, direcionando para a sa√≠da padr√£o e um arquivo.
logging.basicConfig(
    level=logging.INFO, # Define o n√≠vel m√≠nimo de mensagens a serem registradas (INFO, WARNING, ERROR, etc.).
    format='%(asctime)s - ETL - %(levelname)s - %(message)s', # Formato das mensagens de log.
    handlers=[ # Destinos onde as mensagens de log ser√£o enviadas.
        logging.StreamHandler(), # Envia logs para a sa√≠da padr√£o (console/terminal).
        logging.FileHandler('etl.log', mode='a') # Envia logs para o arquivo 'etl.log', adicionando (append) ao final.
    ]
)
logger = logging.getLogger(__name__) # Cria um logger espec√≠fico para este m√≥dulo.

# --- Carregamento de Vari√°veis de Ambiente ---
# Carrega as vari√°veis de ambiente do arquivo .env localizado na raiz do projeto.
# O Path(__file__).parent.parent constr√≥i o caminho para a raiz do projeto.
load_dotenv(Path(__file__).parent.parent / '.env')
logger.info("‚úÖ Vari√°veis de ambiente carregadas do arquivo .env na raiz do projeto.")

# --- Configura√ß√µes e Par√¢metros do Processo ETL ---
# Obt√©m vari√°veis de ambiente, com valores padr√£o caso n√£o estejam definidas.
# REDIS_URL: URL de conex√£o com o servidor Redis, para cache e armazenamento.
REDIS_URL = os.getenv('REDIS_URL', 'redis://localhost:6379')
# COINGECKO_API_KEY: Chave da API do CoinGecko, para acesso a dados premium (opcional).
COINGECKO_API_KEY = os.getenv('COINGECKO_API_KEY')
# UPDATE_INTERVAL: Intervalo em segundos entre cada ciclo de atualiza√ß√£o ETL (padr√£o: 5 minutos).
UPDATE_INTERVAL = int(os.getenv('UPDATE_INTERVAL', '300'))
# Prefixo para as chaves no Redis
REDIS_KEY_PREFIX = 'crypto:'
# Nome da chave para estat√≠sticas de mercado agregadas
MARKET_STATS_KEY = f"{REDIS_KEY_PREFIX}market:stats"

# URL base da API CoinGecko.
API_BASE_URL = "https://api.coingecko.com/api/v3"
# Tempo m√°ximo em segundos para uma requisi√ß√£o HTTP.
REQUEST_TIMEOUT = 30
# N√∫mero m√°ximo de tentativas de retry para requisi√ß√µes com falha.
MAX_RETRIES = 5
# Atraso inicial em segundos antes de uma nova tentativa (com backoff exponencial).
RETRY_DELAY = 10
# Tamanho do lote de registros a serem inseridos/atualizados no banco de dados por vez.
BATCH_SIZE = 100
# Atraso em segundos entre requisi√ß√µes consecutivas √† API CoinGecko para respeitar limites de taxa.
RATE_LIMIT_DELAY = 1.2

# --- Inicializa√ß√£o do Cliente Redis ---
# Inst√¢ncia do cliente Redis, inicializada como None e configurada posteriormente.
redis_client: Optional[aioredis.Redis] = None


class CryptoDataUpdater:
    """
    Classe principal para gerenciar o processo ETL (Extra√ß√£o, Transforma√ß√£o, Carga)
    de dados de criptomoedas. Encapsula toda a l√≥gica de coleta, processamento
    e persist√™ncia de dados.
    """

    def __init__(self):
        """
        Inicializa o CryptoDataUpdater.
        
        Configura o cliente HTTP ass√≠ncrono (httpx) para fazer requisi√ß√µes √† CoinGecko,
        definindo timeout e cabe√ßalhos personalizados. Tamb√©m inicializa um dicion√°rio
        para armazenar m√©tricas de performance do ETL.
        """
        # Cliente HTTP ass√≠ncrono para requisi√ß√µes externas, com timeout e headers.
        self.client = httpx.AsyncClient(
            timeout=REQUEST_TIMEOUT,
            headers={
                'User-Agent': 'CryptoDash-ETL/2.0', # Identifica o cliente para a API externa.
                'Accept': 'application/json',
                'X-Requested-With': 'CryptoDash ETL Process' # Identificador da requisi√ß√£o.
            }
        )
        self.last_update: Optional[datetime] = None # Armazena o timestamp da √∫ltima atualiza√ß√£o bem-sucedida.
        self.stats: Dict[str, Any] = { # Dicion√°rio para coletar m√©tricas de performance.
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_records_processed': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }

    async def init_redis(self) -> None:
        """
        Inicializa a conex√£o com o servidor Redis para ser usado como cache.
        
        Tenta conectar ao Redis usando a URL configurada. Se a conex√£o falhar,
        registra um aviso e desativa o uso do cache para o restante da execu√ß√£o.
        """
        global redis_client # Acessa a vari√°vel global redis_client.
        try:
            # Cria uma inst√¢ncia do cliente Redis ass√≠ncrono.
            redis_client = aioredis.from_url(REDIS_URL, decode_responses=True) # decode_responses para obter strings.
            await redis_client.ping() # Testa a conex√£o com um comando PING.
            logger.info("‚úÖ Conex√£o com Redis estabelecida para cache do ETL.")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Redis n√£o dispon√≠vel em {REDIS_URL}: {e}. Continuando o ETL SEM cache.")
            redis_client = None # Define como None para desativar o cache.

    async def get_cache(self, key: str) -> Optional[str]:
        """
        Tenta obter um valor do cache Redis usando uma chave espec√≠fica.
        
        Args:
            key (str): A chave para buscar no cache.
            
        Returns:
            Optional[str]: O valor armazenado em string se encontrado e Redis estiver ativo, sen√£o None.
        """
        if not redis_client: # Se o cliente Redis n√£o est√° ativo, n√£o h√° cache.
            return None
        try:
            # Retorna o valor associado √† chave. Se a chave n√£o existe, retorna None.
            return await redis_client.get(key)
        except Exception as e:
            logger.warning(f"Erro ao tentar ler do cache Redis (chave: {key}): {e}")
            return None

    async def set_cache(self, key: str, value: str, ttl: int = 300) -> None:
        """
        Armazena um valor no cache Redis com um tempo de vida (TTL).
        
        Args:
            key (str): A chave para armazenar o valor.
            value (str): O valor a ser armazenado.
            ttl (int): Tempo de vida do item no cache em segundos (padr√£o: 300s = 5 minutos).
        """
        if not redis_client: # Se o cliente Redis n√£o est√° ativo, n√£o armazena.
            return
        try:
            # Define a chave com um valor e um tempo de expira√ß√£o (TTL).
            await redis_client.setex(key, ttl, value)
        except Exception as e:
            logger.warning(f"Erro ao tentar salvar no cache Redis (chave: {key}): {e}")

    async def check_rate_limit(self) -> None:
        """
        Pausa a execu√ß√£o para respeitar o limite de taxa (rate limit) da API CoinGecko.
        
        Um atraso √© aplicado entre as requisi√ß√µes para evitar exceder o n√∫mero de chamadas
        permitidas pela API em um determinado per√≠odo.
        """
        await asyncio.sleep(RATE_LIMIT_DELAY) # Pausa por um tempo definido em RATE_LIMIT_DELAY.

    def validate_api_response(self, data: List[Dict[str, Any]]) -> Tuple[bool, str]:
        """
        Valida a estrutura b√°sica e o conte√∫do da resposta recebida da API CoinGecko.
        
        Verifica se a resposta n√£o est√° vazia, √© uma lista e se os primeiros itens
        cont√™m os campos obrigat√≥rios esperados.
        
        Args:
            data (List[Dict[str, Any]]): A lista de dados brutos recebidos da API.
            
        Returns:
            Tuple[bool, str]: Um booleano indicando se a valida√ß√£o foi bem-sucedida e
                              uma mensagem de status ou erro.
        """
        if not data: # Verifica se a lista de dados est√° vazia.
            return False, "Resposta vazia da API"

        if not isinstance(data, list): # Verifica se o tipo de dado √© uma lista.
            return False, "Formato de resposta inv√°lido: esperado uma lista"

        # Campos que s√£o essenciais para o processamento posterior.
        required_fields = ['id', 'symbol', 'name', 'current_price']
        # Valida apenas os primeiros 5 itens para uma verifica√ß√£o r√°pida de sanidade.
        for item in data[:5]: 
            if not all(field in item for field in required_fields): # Verifica se todos os campos obrigat√≥rios est√£o presentes.
                return False, f"Campos obrigat√≥rios ausentes em um item da API: {required_fields}"

        return True, "Valida√ß√£o OK" # Se tudo estiver certo, a valida√ß√£o √© bem-sucedida.

    async def fetch_crypto_data(self) -> List[Dict[str, Any]]:
        """
        üîç FASE 1: EXTRA√á√ÉO - Busca dados de criptomoedas da API CoinGecko com estrat√©gia de cache e retries.
        
        Tenta obter os dados do cache Redis primeiro. Se n√£o estiverem no cache (cache miss),
        realiza uma requisi√ß√£o √† API CoinGecko, aplicando um limite de taxa e l√≥gica de retry
        com backoff exponencial para garantir robustez contra falhas de rede ou limites da API.
        
        Returns:
            List[Dict[str, Any]]: Uma lista de dicion√°rios contendo os dados brutos das criptomoedas.
                                  Retorna uma lista vazia em caso de falha cr√≠tica.
        """
        cache_key = "crypto_data_latest" # Chave para armazenar os dados mais recentes no cache.
        start_time = datetime.utcnow() # Registra o tempo de in√≠cio da extra√ß√£o para m√©tricas.

        # 1. Tenta buscar dados do cache Redis:
        cached_data = await self.get_cache(cache_key)
        if cached_data: # Se os dados foram encontrados no cache...
            logger.info("üìã Dados de criptomoedas obtidos do cache Redis.")
            self.stats['cache_hits'] += 1 # Incrementa o contador de cache hits.
            return json.loads(cached_data) # Retorna os dados desserializados do JSON.

        # 2. Se os dados n√£o est√£o no cache (cache miss), busca na API CoinGecko:
        logger.info("üåê Dados de criptomoedas n√£o encontrados no cache. Buscando frescos da API CoinGecko...")
        self.stats['cache_misses'] += 1 # Incrementa o contador de cache misses.

        # Par√¢metros da requisi√ß√£o para a API CoinGecko.
        params = {
            'vs_currency': 'usd', # Moeda de compara√ß√£o (USD).
            'order': 'market_cap_desc', # Ordenar por capitaliza√ß√£o de mercado decrescente.
            'per_page': 250, # N√∫mero de resultados por p√°gina.
            'page': 1, # Primeira p√°gina de resultados.
            'sparkline': False, # N√£o incluir dados de sparkline.
            'price_change_percentage': '24h,7d,30d', # Incluir varia√ß√£o de pre√ßo para 24h, 7d e 30d.
            'market_data': True, # Incluir dados de mercado.
            'community_data': False, # N√£o incluir dados de comunidade.
            'developer_data': False # N√£o incluir dados de desenvolvedores.
        }

        # Se uma chave de API do CoinGecko for fornecida, usa-a para acesso premium.
        if COINGECKO_API_KEY:
            params['x_cg_demo_api_key'] = COINGECKO_API_KEY # Par√¢metro espec√≠fico para a API CoinGecko.
            logger.info("üîë Usando API key do CoinGecko para acessar dados premium.")

        try:
            # Implementa a l√≥gica de retry com backoff exponencial para maior robustez.
            for attempt in range(MAX_RETRIES): # Tenta um n√∫mero m√°ximo de vezes.
                try:
                    await self.check_rate_limit() # Respeita o rate limit antes de cada requisi√ß√£o.

                    # Faz a requisi√ß√£o GET para o endpoint de mercados de moedas.
                    response = await self.client.get(
                        f"{API_BASE_URL}/coins/markets",
                        params=params
                    )
                    response.raise_for_status() # Levanta um HTTPStatusError para respostas 4xx/5xx.
                    data = response.json() # Desserializa a resposta JSON.

                    # Valida a estrutura da resposta da API.
                    is_valid, validation_msg = self.validate_api_response(data)
                    if not is_valid: # Se a valida√ß√£o falhar, levanta um ValueError.
                        raise ValueError(f"Valida√ß√£o da resposta da API falhou: {validation_msg}")

                    # 3. Armazena os dados extra√≠dos no cache Redis por 5 minutos.
                    await self.set_cache(cache_key, json.dumps(data), 300)

                    elapsed = (datetime.utcnow() - start_time).total_seconds() # Tempo total de extra√ß√£o.
                    logger.info(f"‚úÖ Extra√ß√£o de dados conclu√≠da: {len(data)} registros em {elapsed:.2f} segundos.")
                    self.stats['successful_requests'] += 1 # Incrementa requisi√ß√µes bem-sucedidas.
                    self.stats['total_requests'] += 1 # Incrementa o total de requisi√ß√µes.

                    return data # Retorna os dados extra√≠dos.

                except (httpx.HTTPStatusError, httpx.RequestError, ValueError) as e: # Captura erros de HTTP, requisi√ß√£o ou valida√ß√£o.
                    self.stats['failed_requests'] += 1 # Incrementa requisi√ß√µes falhas.
                    if attempt == MAX_RETRIES - 1: # Se for a √∫ltima tentativa, re-levanta o erro.
                        raise

                    delay = RETRY_DELAY * (2 ** attempt) # Calcula o atraso usando backoff exponencial.
                    logger.warning(
                        f"‚ùå Tentativa {attempt + 1}/{MAX_RETRIES} de extra√ß√£o falhou: {e}. "
                        f"Nova tentativa em {delay:.1f} segundos..."
                    )
                    await asyncio.sleep(delay) # Aguarda antes de tentar novamente.

        except Exception as e:
            logger.error(f"üí• Erro cr√≠tico e irrecuper√°vel na fase de EXTRA√á√ÉO: {str(e)}", exc_info=True)
            raise # Re-levanta o erro cr√≠tico.
            
    def transform_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        üõ†Ô∏è FASE 2: TRANSFORMA√á√ÉO - Processa, valida e limpa os dados brutos de criptomoedas.
        
        Itera sobre a lista de dados brutos, aplicando valida√ß√µes essenciais, limpando
        e normalizando os valores (ex: convertendo para float, formatando strings).
        Adiciona metadados de processamento e calcula um score de qualidade para cada item.
        
        Args:
            data (List[Dict[str, Any]]): A lista de dicion√°rios com os dados brutos da API.
            
        Returns:
            List[Dict[str, Any]]: Uma lista de dicion√°rios com os dados transformados e prontos para a carga.
        """
        logger.info(f"üîÑ Iniciando transforma√ß√£o de {len(data)} registros...")
        start_time = datetime.utcnow() # Registra o tempo de in√≠cio da transforma√ß√£o.

        transformed_data = [] # Lista para armazenar os dados transformados.
        errors = [] # Lista para registrar quaisquer erros durante a transforma√ß√£o.

        for i, item in enumerate(data): # Itera sobre cada item nos dados brutos.
            try:
                # 1. Valida√ß√£o de dados essenciais para cada item.
                if not self._validate_crypto_item(item): # Usa um m√©todo auxiliar para valida√ß√£o.
                    errors.append(f"Item {i} (ID: {item.get('id', 'N/A')}): dados inv√°lidos ou incompletos.")
                    continue # Pula para o pr√≥ximo item se a valida√ß√£o falhar.

                # 2. Limpeza e normaliza√ß√£o dos dados.
                clean_item = self._clean_crypto_data(item) # Usa um m√©todo auxiliar para limpeza.

                # 3. Adiciona metadados de processamento ao item transformado.
                clean_item.update({
                    'last_updated_from_api': item.get('last_updated'), # Timestamp original da API, se dispon√≠vel.
                    'processed_at': datetime.utcnow().isoformat() + 'Z', # Timestamp de quando foi processado.
                    'data_quality_score': self._calculate_quality_score(item) # Score de qualidade dos dados.
                })

                transformed_data.append(clean_item) # Adiciona o item transformado √† lista.

            except Exception as e:
                # Captura e registra erros espec√≠ficos de transforma√ß√£o de um item.
                errors.append(f"Item {i} (ID: {item.get('id', 'N/A')}): erro de processamento - {str(e)}")
                logger.warning(f"‚ö†Ô∏è Erro processando item {i} (ID: {item.get('id', 'N/A')}): {e}")

        # 4. Log de estat√≠sticas da transforma√ß√£o.
        elapsed = (datetime.utcnow() - start_time).total_seconds() # Tempo total da transforma√ß√£o.
        success_rate = (len(transformed_data) / len(data)) * 100 if data else 0 # Taxa de sucesso.

        logger.info(
            f"‚úÖ Transforma√ß√£o conclu√≠da: {len(transformed_data)}/{len(data)} registros v√°lidos "
            f"({success_rate:.1f}%) em {elapsed:.2f} segundos."
        )

        if errors:
            logger.warning(f"‚ö†Ô∏è Foram encontrados {len(errors)} erros durante a transforma√ß√£o. Primeiros 5: {errors[:5]}...")

        return transformed_data # Retorna a lista de dados transformados.

    def _validate_crypto_item(self, item: Dict[str, Any]) -> bool:
        """
        M√©todo auxiliar para validar se um item de criptomoeda possui os campos obrigat√≥rios.
        
        Args:
            item (Dict[str, Any]): O dicion√°rio representando um item de criptomoeda.
            
        Returns:
            bool: True se todos os campos obrigat√≥rios estiverem presentes e n√£o forem None, False caso contr√°rio.
        """
        required_fields = ['id', 'symbol', 'name', 'current_price'] # Campos que devem existir.
        return all(item.get(field) is not None for field in required_fields) # Verifica se todos est√£o presentes e n√£o nulos.

    def _clean_crypto_data(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """
        M√©todo auxiliar para limpar, normalizar e mapear os dados de criptomoeda.
        
        Converte tipos de dados (strings para floats/ints com seguran√ßa), padroniza
        valores (ex: min√∫sculas para s√≠mbolo/ID) e seleciona os campos relevantes
        para o esquema do banco de dados.
        
        Args:
            item (Dict[str, Any]): O dicion√°rio de dados brutos de uma criptomoeda.
            
        Returns:
            Dict[str, Any]: Um novo dicion√°rio com os dados limpos e formatados.
        """
        # Fun√ß√µes auxiliares para convers√£o segura de tipos num√©ricos.
        def safe_float(value, default=0.0): # Converte para float, retorna default em caso de erro.
            if value is None or value == '':
                return default
            try:
                return float(value)
            except (ValueError, TypeError):
                return default

        def safe_int(value, default=0): # Converte para int, com cuidado para floats, retorna default em caso de erro.
            if value is None or value == '':
                return default
            try:
                # Tenta converter para float primeiro para lidar com n√∫meros como "123.0", depois para int.
                return int(float(value)) if '.' not in str(value) else default
            except (ValueError, TypeError):
                return default

        # Retorna um dicion√°rio com os dados mapeados e limpos para o esquema final.
        return {
            'id': str(item.get('id', '')).lower(), # ID da criptomoeda em min√∫sculas.
            'symbol': str(item.get('symbol', '')).lower(), # S√≠mbolo em min√∫sculas.
            'name': str(item.get('name', '')).strip(), # Nome, remove espa√ßos extras.
            'price': safe_float(item.get('current_price'), 0.0), # Pre√ßo atual.
            'market_cap': safe_float(item.get('market_cap')), # Capitaliza√ß√£o de mercado.
            'volume_24h': safe_float(item.get('total_volume')), # Volume de 24h.
            'change_24h': safe_float(item.get('price_change_percentage_24h')) / 100,  # Varia√ß√£o % de 24h (convertida para decimal).
            'last_updated': datetime.utcnow().isoformat() + 'Z', # Timestamp da √∫ltima atualiza√ß√£o (no script).

            # Campos adicionais de dados, limpos e formatados.
            'high_24h': safe_float(item.get('high_24h')),
            'low_24h': safe_float(item.get('low_24h')),
            'price_change_24h': safe_float(item.get('price_change_24h')),
            'market_cap_change_24h': safe_float(item.get('market_cap_change_24h')),
            'market_cap_change_percentage_24h': safe_float(item.get('market_cap_change_percentage_24h')),
            'circulating_supply': safe_float(item.get('circulating_supply')),
            'total_supply': safe_float(item.get('total_supply')),
            'max_supply': safe_float(item.get('max_supply')),

            # Dados de ATH (All-Time High) / ATL (All-Time Low).
            'ath': safe_float(item.get('ath')),
            'ath_change_percentage': safe_float(item.get('ath_change_percentage')),
            'ath_date': item.get('ath_date'),
            'atl': safe_float(item.get('atl')),
            'atl_change_percentage': safe_float(item.get('atl_change_percentage')),
            'atl_date': item.get('atl_date'),

            # Metadados adicionais.
            'image_url': str(item.get('image', '')).strip(), # URL da imagem, remove espa√ßos.
            'market_cap_rank': safe_int(item.get('market_cap_rank')), # Rank de capitaliza√ß√£o de mercado.
            'roi': item.get('roi'),  # Retorna ROI como est√° (pode ser JSON ou None).
        }

    def _calculate_quality_score(self, item: Dict[str, Any]) -> float:
        """
        M√©todo auxiliar para calcular um score de qualidade para um item de dados.
        
        Atribui um score inicial de 100 e subtrai pontos para cada campo essencial
        que est√° ausente ou √© nulo, dando uma indica√ß√£o da completude e qualidade dos dados.
        
        Args:
            item (Dict[str, Any]): O dicion√°rio de dados de uma criptomoeda.
            
        Returns:
            float: Um score de qualidade entre 0 e 100.
        """
        score = 100.0 # Score inicial m√°ximo.
        # Define os campos a serem verificados e seu 'peso' na redu√ß√£o do score.
        checks = [
            ('current_price', 5),
            ('market_cap', 10),
            ('total_volume', 5),
            ('price_change_percentage_24h', 5),
            ('image', 2),
            ('market_cap_rank', 3),
        ]

        for field, weight in checks: # Itera sobre os campos e seus pesos.
            if not item.get(field): # Se o campo est√° ausente ou √© None...
                score -= weight # ...reduz o score.

        return max(0, score) # Garante que o score n√£o seja negativo.
    
    async def update_database(self, data: List[Dict[str, Any]]) -> None:
        """
        üíæ FASE 3: ARMAZENAMENTO - Armazena os dados processados no Redis.
        
        Processa os dados em lotes e armazena no Redis, utilizando hashes para estruturar
        os dados de forma eficiente. Inclui l√≥gica de retry para garantir a persist√™ncia 
        dos dados mesmo com falhas transit√≥rias.
        
        Args:
            data (List[Dict[str, Any]]): Lista de dicion√°rios com os dados de criptomoedas processados.
        """
        if not data:  # Se n√£o houver dados para armazenar, sai da fun√ß√£o.
            logger.warning("‚ö†Ô∏è Nenhum dado para armazenar no Redis.")
            return

        logger.info(f"üíæ Iniciando armazenamento no Redis: {len(data)} registros...")
        start_time = datetime.utcnow()  # Registra o tempo de in√≠cio do armazenamento.

        # Tamanho do lote para opera√ß√µes de armazenamento
        batch_size = 100
        total_batches = (len(data) + batch_size - 1) // batch_size
        success_count = 0
        failed_count = 0

        # Processa cada lote de dados
        for batch_num, i in enumerate(range(0, len(data), batch_size), 1):
            try:
                batch = data[i:i + batch_size]
                logger.info(f"üîÑ Processando lote {batch_num}/{total_batches} ({len(batch)} registros)...")

                # Prepara os dados para armazenamento no Redis
                redis_data = {}
                for item in batch:
                    crypto_id = item.get('id')
                    if not crypto_id:
                        continue
                        
                    # Cria uma chave √∫nica para cada criptomoeda
                    key = f"{REDIS_KEY_PREFIX}price:{crypto_id.lower()}"
                    
                    # Prepara os dados para armazenamento
                    redis_data[key] = json.dumps({
                        'id': crypto_id,
                        'symbol': item.get('symbol', '').upper(),
                        'name': item.get('name', ''),
                        'current_price': item.get('current_price', 0),
                        'price_change_percentage_24h': item.get('price_change_percentage_24h', 0),
                        'market_cap': item.get('market_cap', 0),
                        'total_volume': item.get('total_volume', 0),
                        'image': item.get('image', ''),
                        'last_updated': item.get('last_updated', datetime.utcnow().isoformat()),
                        'high_24h': item.get('high_24h', 0),
                        'low_24h': item.get('low_24h', 0),
                        'price_change_24h': item.get('price_change_24h', 0),
                        'market_cap_change_24h': item.get('market_cap_change_24h', 0),
                        'market_cap_change_percentage_24h': item.get('market_cap_change_percentage_24h', 0),
                        'circulating_supply': item.get('circulating_supply', 0),
                        'total_supply': item.get('total_supply', 0),
                        'max_supply': item.get('max_supply', 0),
                        'ath': item.get('ath', 0),
                        'ath_change_percentage': item.get('ath_change_percentage', 0),
                        'ath_date': item.get('ath_date', ''),
                        'atl': item.get('atl', 0),
                        'atl_change_percentage': item.get('atl_change_percentage', 0),
                        'atl_date': item.get('atl_date', '')
                    })
                
                # Armazena os dados no Redis
                success = await self._store_in_redis(redis_data)
                
                if success:
                    success_count += len(batch)
                    logger.info(f"‚úÖ Lote {batch_num} armazenado com sucesso no Redis.")
                else:
                    failed_count += len(batch)
                    logger.error(f"‚ùå Falha ao armazenar o lote {batch_num}/{total_batches} no Redis.")

                # Pequeno atraso entre lotes para n√£o sobrecarregar o Redis
                if batch_num < total_batches:
                    await asyncio.sleep(0.1)

            except Exception as e:
                failed_count += len(batch)
                logger.error(f"‚ùå Erro inesperado ao processar o lote {batch_num}/{total_batches}: {e}", exc_info=True)

        # Calcula m√©tricas de desempenho
        elapsed = (datetime.utcnow() - start_time).total_seconds()
        success_rate = (success_count / len(data)) * 100 if data else 0

        logger.info("=" * 60)
        logger.info("üìä ESTAT√çSTICAS DE ARMAZENAMENTO NO REDIS:")
        logger.info(f"   ‚Ä¢ Total de registros para armazenar: {len(data)}")
        logger.info(f"   ‚Ä¢ Lotes processados: {total_batches}")
        logger.info(f"   ‚Ä¢ Registros com sucesso: {success_count}")
        logger.info(f"   ‚Ä¢ Registros com falha: {failed_count}")
        logger.info(f"   ‚Ä¢ Taxa de sucesso: {success_rate:.2f}%")
        logger.info(f"   ‚Ä¢ Tempo total de processamento: {elapsed:.2f} segundos")
        logger.info(f"   ‚Ä¢ Performance m√©dia: {len(data)/elapsed:.1f} registros/segundo" if elapsed > 0 else "")
        logger.info("=" * 60)

        # Atualiza as estat√≠sticas
        self.stats['total_records_processed'] += success_count
        self.stats['failed_requests'] += failed_count

    async def _store_in_redis(self, data: Dict[str, str]) -> bool:
        """
        Armazena os dados no Redis usando pipeline para melhor desempenho.
        
        Args:
            data (Dict[str, str]): Dicion√°rio onde as chaves s√£o as chaves do Redis
                                 e os valores s√£o strings JSON com os dados a serem armazenados.
        
        Returns:
            bool: True se o armazenamento for bem-sucedido, False caso contr√°rio.
        """
        if not redis_client:
            logger.error("‚ùå Cliente Redis n√£o inicializado. N√£o foi poss√≠vel armazenar os dados.")
            return False
            
        max_retries = 3
        base_delay = 1.0
        
        for attempt in range(max_retries):
            try:
                # Cria um pipeline para executar m√∫ltiplos comandos em uma √∫nica opera√ß√£o
                async with redis_client.pipeline(transaction=True) as pipe:
                    for key, value in data.items():
                        # Define o valor e o tempo de expira√ß√£o (1 hora)
                        pipe.set(key, value, ex=3600)
                    
                    # Executa o pipeline
                    await pipe.execute()
                
                return True
                
            except Exception as e:
                if attempt == max_retries - 1:  # √öltima tentativa
                    logger.error(f"‚ùå Falha ao armazenar dados no Redis ap√≥s {max_retries} tentativas: {e}", exc_info=True)
                    return False
                    
                # Backoff exponencial
                delay = base_delay * (2 ** attempt)
                logger.warning(f"‚ö†Ô∏è Tentativa {attempt + 1}/{max_retries} falhou. Nova tentativa em {delay:.1f}s...")
                await asyncio.sleep(delay)
        
        return False

    
    async def run_update(self) -> None:
        """
        üöÄ Executa um ciclo completo do processo ETL (Extra√ß√£o, Transforma√ß√£o, Carga).
        
        Orquestra as tr√™s fases principais do ETL, registra m√©tricas de tempo e processamento,
        e trata exce√ß√µes cr√≠ticas que possam ocorrer durante qualquer fase.
        
        Raises:
            Exception: Re-levanta qualquer erro cr√≠tico que impe√ßa o ETL de continuar.
        """
        start_time = datetime.utcnow() # Registra o tempo de in√≠cio do ciclo ETL.
        logger.info("üéØ Iniciando um ciclo completo do processo ETL...")

        try:
            # --- FASE 1: EXTRA√á√ÉO ---
            logger.info("üì• === INICIANDO FASE 1: EXTRA√á√ÉO DE DADOS ===")
            data = await self.fetch_crypto_data() # Chama o m√©todo de extra√ß√£o.

            if not data: # Se a extra√ß√£o n√£o retornou dados, encerra o ciclo.
                logger.warning("‚ö†Ô∏è NENHUM DADO RETORNADO DA API. Encerrando ciclo ETL.")
                return

            # --- FASE 2: TRANSFORMA√á√ÉO ---
            logger.info("üîÑ === INICIANDO FASE 2: TRANSFORMA√á√ÉO DE DADOS ===")
            transformed_data = self.transform_data(data) # Chama o m√©todo de transforma√ß√£o.

            if not transformed_data: # Se a transforma√ß√£o n√£o resultou em dados v√°lidos, encerra.
                logger.error("‚ùå FALHA NA TRANSFORMA√á√ÉO: Nenhum dado v√°lido ap√≥s o processamento. Encerrando ciclo ETL.")
                return

            # --- FASE 3: CARGA ---
            logger.info("üíæ === INICIANDO FASE 3: CARGA DE DADOS NO SUPABASE ===")
            await self.update_database(transformed_data) # Chama o m√©todo de carga.

            # --- ATUALIZA√á√ÉO DE ESTAT√çSTICAS GLOBAIS DE MERCADO ---
            # Este m√©todo calcula e insere estat√≠sticas agregadas em uma tabela separada.
            logger.info("üìä Atualizando estat√≠sticas globais de mercado...")
            await self.update_market_stats(data) # Usa os dados brutos para calcular estat√≠sticas.

            # --- M√âTRICAS FINAIS DO CICLO ETL ---
            total_time = (datetime.utcnow() - start_time).total_seconds() # Tempo total do ciclo.

            logger.info("üéâ === CICLO ETL CONCLU√çDO COM SUCESSO ===")
            logger.info(f"‚è±Ô∏è Tempo total do ciclo ETL: {total_time:.2f} segundos")
            logger.info(f"üìä Taxa de processamento: {len(transformed_data)/total_time:.1f} registros/segundo")
            logger.info(f"üéØ Efici√™ncia: {len(transformed_data)/len(data)*100:.1f}% dos dados originais processados.")

            # Registra todas as m√©tricas de performance acumuladas.
            self.log_performance_metrics()

        except Exception as e: # Captura qualquer erro cr√≠tico no processo ETL.
            logger.error(f"üí• Erro cr√≠tico no processo ETL: {str(e)}", exc_info=True)
            raise # Re-levanta o erro para tratamento externo, se houver.

    async def update_market_stats(self, data: List[Dict[str, Any]]) -> None:
        """
        üìä Atualiza estat√≠sticas globais de mercado no banco de dados Supabase.
        
        Calcula m√©tricas agregadas como capitaliza√ß√£o de mercado total, volume de 24h,
        n√∫mero de criptomoedas ativas e distribui√ß√£o por capitaliza√ß√£o. Essas estat√≠sticas
        s√£o ent√£o inseridas em uma tabela separada no Supabase.
        
        Args:
            data (List[Dict[str, Any]]): A lista de dicion√°rios com os dados brutos das criptomoedas.
        """
        if not supabase_client: # Garante que o cliente Supabase esteja inicializado.
            logger.error("‚ùå Cliente Supabase n√£o inicializado. N√£o foi poss√≠vel atualizar as estat√≠sticas de mercado.")
            return

        try:
            logger.info("üîÑ Calculando e atualizando estat√≠sticas globais de mercado...")

            # 1. Calcula estat√≠sticas agregadas a partir dos dados extra√≠dos.
            total_market_cap = sum(item.get('market_cap', 0) for item in data if item.get('market_cap'))
            total_volume_24h = sum(item.get('total_volume', 0) for item in data if item.get('total_volume'))
            active_cryptocurrencies = len([item for item in data if item.get('market_cap', 0) > 0])

            # 2. Calcula a distribui√ß√£o de capitaliza√ß√£o de mercado (exemplo).
            market_cap_distribution = {
                'large_cap': len([item for item in data if item.get('market_cap', 0) > 10000000000]),  # > $10 bilh√µes
                'mid_cap': len([item for item in data if 1000000000 < item.get('market_cap', 0) <= 10000000000]),  # $1 bilh√£o - $10 bilh√µes
                'small_cap': len([item for item in data if item.get('market_cap', 0) <= 1000000000])  # < $1 bilh√£o
            }

            # 3. Monta o dicion√°rio de estat√≠sticas de mercado.
            market_stats = {
                'timestamp': datetime.utcnow().isoformat() + 'Z', # Timestamp da atualiza√ß√£o.
                'active_cryptocurrencies': active_cryptocurrencies, # N√∫mero de criptomoedas ativas.
                'total_market_cap_usd': total_market_cap, # Capitaliza√ß√£o total de mercado.
                'total_volume_usd_24h': total_volume_24h, # Volume total em 24h.
                'market_cap_percentage': market_cap_distribution, # Distribui√ß√£o de capitaliza√ß√£o (JSON).
                'markets': 1  # N√∫mero de mercados (simulado, aqui √© a API CoinGecko).
            }

            # 4. Insere as estat√≠sticas na tabela 'market_stats' do Supabase.
            result = supabase_client.table('market_stats').insert(market_stats).execute()

            # Verifica se houve erro na opera√ß√£o de inser√ß√£o.
            if hasattr(result, 'error') and result.error: # 'hasattr' √© usado para verificar a exist√™ncia de 'error'
                logger.warning(f"‚ö†Ô∏è Erro ao atualizar a tabela 'market_stats': {result.error}")
            else:
                logger.info("‚úÖ Estat√≠sticas globais de mercado atualizadas com sucesso.")

        except Exception as e: # Captura e registra qualquer erro durante a atualiza√ß√£o das estat√≠sticas.
            logger.warning(f"‚ö†Ô∏è Erro inesperado ao calcular/atualizar market_stats: {e}", exc_info=True)

    def log_performance_metrics(self) -> None:
        """
        üìà Log detalhado de m√©tricas de performance acumuladas do ETL.
        
        Exibe no log um resumo das requisi√ß√µes, registros processados, cache hits e misses,
        oferecendo uma vis√£o abrangente da efici√™ncia do processo ETL ao longo de sua execu√ß√£o.
        """
        logger.info("üìä === M√âTRICAS DE PERFORMANCE ACUMULADAS DO ETL ===")
        logger.info(f"üîÑ Total de requisi√ß√µes √† API: {self.stats['total_requests']}")
        logger.info(f"‚úÖ Requisi√ß√µes bem-sucedidas: {self.stats['successful_requests']}")
        logger.info(f"‚ùå Requisi√ß√µes com falha: {self.stats['failed_requests']}")
        # Calcula a taxa de sucesso das requisi√ß√µes, evitando divis√£o por zero.
        logger.info(f"üìà Taxa de sucesso das requisi√ß√µes: {self.stats['successful_requests']/max(self.stats['total_requests'],1)*100:.1f}%")
        logger.info(f"üíæ Total de registros de criptomoedas processados e carregados: {self.stats['total_records_processed']}")
        logger.info(f"üéØ Cache hits (dados obtidos do Redis): {self.stats['cache_hits']}")
        logger.info(f"üí≠ Cache misses (dados buscados da API): {self.stats['cache_misses']}")
        logger.info("=" * 50)

    async def run_continuous(self) -> None:
        """
        üîÑ Executa o processo ETL continuamente em um loop, com um intervalo configur√°vel.
        
        Este √© o modo de opera√ß√£o principal para manter os dados atualizados. Ele gerencia
        os ciclos de atualiza√ß√£o, respeitando o intervalo definido e tratando interrup√ß√µes
        (como Ctrl+C) e erros inesperados para garantir resili√™ncia.
        """
        logger.info("üéØ Iniciando o servi√ßo ETL em modo cont√≠nuo...")
        logger.info(f"‚è∞ O intervalo de atualiza√ß√£o configurado √© de {UPDATE_INTERVAL} segundos.")

        await self.init_redis() # Inicializa a conex√£o com o Redis no in√≠cio do servi√ßo cont√≠nuo.

        cycle_count = 0 # Contador para o n√∫mero de ciclos ETL executados.

        while True: # Loop infinito para execu√ß√£o cont√≠nua.
            try:
                cycle_count += 1 # Incrementa o contador de ciclo.
                cycle_start = datetime.utcnow() # Registra o tempo de in√≠cio do ciclo atual.

                logger.info(f"üîÑ === INICIANDO CICLO ETL #{cycle_count} ===")

                await self.run_update() # Executa um ciclo completo de extra√ß√£o, transforma√ß√£o e carga.

                cycle_time = (datetime.utcnow() - cycle_start).total_seconds() # Calcula o tempo que o ciclo levou.

                # Calcula o tempo que o script deve 'dormir' at√© a pr√≥xima execu√ß√£o.
                # Garante que o intervalo entre as atualiza√ß√µes seja respeitado.
                sleep_time = max(0, UPDATE_INTERVAL - cycle_time)

                if sleep_time > 0: # Se o ciclo terminou antes do intervalo, espera.
                    logger.info(f"‚è∞ Pr√≥xima atualiza√ß√£o em {sleep_time:.1f} segundos...")
                    await asyncio.sleep(sleep_time)
                else: # Se o ciclo demorou mais que o intervalo configurado.
                    logger.warning("‚ö†Ô∏è O ciclo ETL demorou mais que o intervalo configurado. A pr√≥xima atualiza√ß√£o ser√° imediata.")

            except asyncio.CancelledError: # Captura o erro quando o loop √© cancelado (ex: por Ctrl+C).
                logger.info("üõë Sinal de cancelamento recebido. Encerrando o servi√ßo ETL cont√≠nuo...")
                break # Sai do loop.

            except Exception as e: # Captura qualquer erro inesperado no ciclo ETL.
                logger.error(f"üí• Erro inesperado no ciclo ETL #{cycle_count}: {str(e)}", exc_info=True)
                logger.info(f"üîÑ Tentando novamente em {RETRY_DELAY} segundos ap√≥s o erro...")
                await asyncio.sleep(RETRY_DELAY) # Aguarda antes de tentar novamente.

    async def close(self) -> None:
        """
        üßπ Fecha todos os recursos abertos pela classe CryptoDataUpdater.
        
        Garante que o cliente HTTP (httpx) e a conex√£o Redis sejam fechados
        de forma limpa ao final da execu√ß√£o do script, liberando recursos do sistema.
        """
        if self.client: # Fecha o cliente HTTP se estiver aberto.
            await self.client.aclose()
        if redis_client: # Fecha a conex√£o Redis se estiver ativa.
            await redis_client.close()
        logger.info("üßπ Todos os recursos de ETL foram liberados com sucesso.")


async def main():
    """
    üèÉ Fun√ß√£o principal que orquestra a execu√ß√£o do script ETL.
    
    Inicializa o `CryptoDataUpdater` e gerencia a execu√ß√£o do ETL em modo cont√≠nuo 
    ou de √∫nica atualiza√ß√£o, tratando interrup√ß√µes do usu√°rio e erros fatais.
    """
    updater = CryptoDataUpdater() # Cria uma inst√¢ncia da classe principal do ETL.
    
    # Inicializa o cliente Redis
    await updater.init_redis()

    try:
        logger.info("üöÄ === INICIANDO CRYPTO DASHBOARD ETL v2.0 ===")
        logger.info("üìä Sistema de extra√ß√£o, transforma√ß√£o e carga para monitoramento de criptomoedas.")
        logger.info("üõ†Ô∏è Processo ETL robusto e otimizado.")

        # Decide se executa o ETL uma √∫nica vez ou em modo cont√≠nuo, baseado no UPDATE_INTERVAL.
        if UPDATE_INTERVAL <= 0: # Se o intervalo for 0 ou negativo, executa uma √∫nica vez.
            logger.info("Configurado para uma √öNICA atualiza√ß√£o de dados (UPDATE_INTERVAL <= 0).")
            await updater.run_update()
        else: # Se o intervalo for positivo, executa em modo cont√≠nuo.
            logger.info(f"Configurado para execu√ß√£o CONT√çNUA com intervalo de {UPDATE_INTERVAL} segundos.")
            await updater.run_continuous()

    except KeyboardInterrupt: # Captura a interrup√ß√£o pelo teclado (Ctrl+C).
        logger.info("üõë Interrup√ß√£o pelo usu√°rio (Ctrl+C) detectada. Iniciando o desligamento do servi√ßo...")
    except Exception as e: # Captura qualquer exce√ß√£o fatal n√£o tratada.
        logger.critical(f"üí• ERRO FATAL E INESPERADO: {str(e)}. Encerrando o script com falha.", exc_info=True)
        sys.exit(1) # Sai do script com c√≥digo de erro 1.
    finally:
        await updater.close() # Garante que os recursos sejam fechados independentemente do resultado.
        logger.info("‚úÖ Servi√ßo ETL encerrado com sucesso. Recursos liberados.")


if __name__ == "__main__":
    # Ponto de entrada do script. Executa a fun√ß√£o 'main' de forma ass√≠ncrona.
    # asyncio.run() √© usado para rodar fun√ß√µes ass√≠ncronas (async def) em Python.
    asyncio.run(main())
